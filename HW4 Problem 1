import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt

#load the cancer dataset
data = pd.read_csv("cancer.csv")

#define the features and parameters
X = data[['radius_mean','texture_mean','perimeter_mean','area_mean','smoothness_mean','compactness_mean','concavity_mean','concave points_mean','symmetry_mean','fractal_dimension_mean','radius_se','texture_se','perimeter_se','area_se','smoothness_se','compactness_se','concavity_se','concave points_se','symmetry_se','fractal_dimension_se','radius_worst','texture_worst','perimeter_worst','area_worst','smoothness_worst','compactness_worst','concavity_worst','concave points_worst','symmetry_worst','fractal_dimension_worst']]
y = data[['id']]

#split the dataset into training and testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)



scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


#find the optimum number of princple component K

K_values = np.arange(1, X_train.shape[1] + 1)
accuracy_scores = []

for K in K_values:
  pca = PCA(n_components=K)
  X_train_pca = pca.fit_transform(X_train)
  X_test_pca = pca.transform(X_test)

  svm = SVC(kernel = 'linear')
  svm.fit(X_train_pca, y_train)

  y_pred = svm.predict(X_test_pca)

  accuracy = accuracy_score(y_test, y_pred)
  accuracy_scores.append(accuracy)

optimal_K = K_values[np.argmax(accuracy_scores)]
print("optimum number of principle components (K): ", optimal_K)


precision_scores = []
recall_scores = []

for K in K_values:
  pca = PCA(n_components=K)
  X_train_pca = pca.fit_transform(X_train)
  X_test_pca = pca.transform(X_test)

  svm = SVC(kernel = 'linear')
  svm.fit(X_train_pca, y_train)

  y_pred = svm.predict(X_test_pca)

  precision = precision_score(y_test, y_pred, average = 'macro')
  recall = recall_score(y_test, y_pred, average = 'macro')

  precision_scores.append(precision)
  recall_scores.append(recall)

#plot
plt.figure(figsize=(10, 6))
plt.plot(K_values, accuracy_scores, label='Accuracy')
plt.plot(K_values, precision_scores, label='Precision')
plt.plot(K_values, recall_scores, label='Recall')
plt.xlabel('Number of Principal Components (K)')
plt.ylabel('Score')
plt.title('Performance Metrics')
plt.legend()
plt.show()

kernels = ['linear', 'poly', 'rbf']
svm_accuracies = []
#implementation of kernel tricks to capture non-linearities within the data
for kernel in kernels:
    pca = PCA(n_components=optimal_K)  # Use PCA with optimal K
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)

    svm = SVC(kernel=kernel)
    svm.fit(X_train_pca, y_train)
    y_pred = svm.predict(X_test_pca)
    accuracy = accuracy_score(y_test, y_pred)
    svm_accuracies.append(accuracy)

#logistic regression for X and Y
logreg = LogisticRegression()
logreg.fit(X_train, y_train)
y_pred_logreg = logreg.predict(X_test)
logreg_accuracy = accuracy_score(y_test, y_pred_logreg)

print("SVM Accuracies:", svm_accuracies)
print("Logistic Regression Accuracy:", logreg_accuracy)
